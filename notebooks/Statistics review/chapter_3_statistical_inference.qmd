
---
title: "Chapter 3: Statistical Inference"
format: 
  pdf:
    toc: true
    number-sections: true
    number-depth: 2
    fig-cap-location: bottom
    fig-align: center
    keep-tex: true
    link-citations: true
    reference-location: document
    citecolor: blue
---

# 3. Statistical Inference (Mathematical Foundations)

This chapter covers how to learn from data — estimating parameters, testing hypotheses, and quantifying uncertainty — using mathematical tools like likelihoods, estimators, and sampling distributions.

## 3.1 Point Estimation

### Definitions

Let $X_1, \dots, X_n \sim F_\theta$ be i.i.d. from a parametric family with unknown parameter $\theta \in \Theta$.  
A point estimator $\hat{\theta}_n = T(X_1, \dots, X_n)$ is a function of the sample that estimates $\theta$.

### Desirable Properties

- **Unbiasedness**:  
  $$\mathbb{E}[\hat{\theta}_n] = \theta$$

- **Consistency**:  
  $$\hat{\theta}_n \xrightarrow{p} \theta \quad \text{as } n \to \infty$$

- **Efficiency**:  
  Minimum variance among all unbiased estimators.

- **Asymptotic Normality**:  
  $$\sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$$

## 3.2 Maximum Likelihood Estimation (MLE)

Given likelihood:  
$$L(\theta; x_1, \dots, x_n) = \prod_{i=1}^n f(x_i; \theta)$$

Log-likelihood:  
$$\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log f(x_i; \theta)$$

MLE is:  
$$\hat{\theta}_{\text{MLE}} = \arg\max_\theta \ell(\theta)$$

### Properties of MLE

- Asymptotically normal and efficient.
- Invariance property: $\hat{g(\theta)} = g(\hat{\theta})$

## 3.3 Confidence Intervals

Let $\hat{\theta}_n$ be an estimator of $\theta$ with standard error $\hat{\sigma}$.

$$(1 - \alpha)\text{-CI}: \hat{\theta}_n \pm z_{\alpha/2} \cdot \hat{\sigma}$$

From asymptotic normality:  
$$\frac{\hat{\theta}_n - \theta}{\hat{\sigma}} \approx \mathcal{N}(0, 1)$$

## 3.4 Hypothesis Testing

### Basic Setup

- Null: $H_0: \theta = \theta_0$
- Alt: $H_1: \theta \neq \theta_0$
- Critical region $R$ such that $P_{\theta_0}(T_n \in R) = \alpha$

### Type I and II Errors

- Type I: Reject $H_0$ when true, probability $\alpha$
- Type II: Fail to reject $H_0$ when false, probability $\beta$
- Power: $1 - \beta$

### Z-test (known variance)

$$Z = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)$$

### t-test (unknown variance)

$$t = \frac{\bar{X} - \mu_0}{s/\sqrt{n}} \sim t_{n-1}$$

## 3.5 Likelihood Ratio Tests (LRT)

Let $\ell_0$, $\ell_1$ be log-likelihoods under $H_0$ and $H_1$:

$$\Lambda = -2(\ell_0 - \ell_1)$$

Under regularity:  
$$\Lambda \xrightarrow{d} \chi^2_k \quad (k = \text{number of restrictions})$$

## 3.6 Asymptotic Theory & Efficiency

### Fisher Information

$$\mathcal{I}(\theta) = \mathbb{E}\left[\left(\frac{\partial}{\partial \theta} \log f(X; \theta)\right)^2\right]$$

### Cramér–Rao Lower Bound

If $\hat{\theta}$ is unbiased:  
$$\text{Var}(\hat{\theta}) \geq \frac{1}{n \cdot \mathcal{I}(\theta)}$$

Equality holds for efficient estimators.
