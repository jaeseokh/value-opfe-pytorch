
---
title: "Chapter 2: Probability and Distributions"
format: 
  pdf:
    toc: true
    number-sections: true
    number-depth: 2
    fig-cap-location: bottom
    fig-align: center
    keep-tex: true
    link-citations: true
    reference-location: document
    citecolor: blue
---

# 2. Probability and Distributions (Mathematical Foundations)

This chapter builds the mathematical backbone of statistical thinking by introducing the concepts of random variables, probability spaces, and distribution theory.

## 2.1 Probability Foundations

### Probability Space

A probability space is defined as a triplet $ (\Omega, \mathcal{F}, P) $, where:

- $ \Omega $: sample space (all possible outcomes)  
- $ \mathcal{F} $: σ-algebra of events (subset of $ \Omega $)  
- $ P $: probability measure such that:

$$
P: \mathcal{F} \to [0, 1], \quad P(\Omega) = 1
$$

### Axioms of Probability (Kolmogorov)

1. $ P(A) \geq 0 $
2. $ P(\Omega) = 1 $
3. For disjoint events $ A_i $:  
$$
P\left(\bigcup_i A_i\right) = \sum_i P(A_i)
$$

### Conditional Probability

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0
$$

### Law of Total Probability

If $ \{B_i\} $ partitions $ \Omega $, then:

$$
P(A) = \sum_i P(A \mid B_i) P(B_i)
$$

### Bayes’ Theorem

$$
P(B_j \mid A) = \frac{P(A \mid B_j) P(B_j)}{\sum_i P(A \mid B_i) P(B_i)}
$$

## 2.2 Random Variables

### Definition

A random variable $ X: \Omega \to \mathbb{R} $ is a measurable function.

### Cumulative Distribution Function (CDF)

$$
F_X(x) = P(X \leq x)
$$

Properties:
- Non-decreasing
- Right-continuous
- $ \lim_{x \to -\infty} F_X(x) = 0 $, $ \lim_{x \to \infty} F_X(x) = 1 $

## 2.3 Probability Distributions

### Discrete Case

Probability mass function (pmf):
$$
p(x_i) = P(X = x_i), \quad \sum_i p(x_i) = 1
$$

Examples:
- Bernoulli($ p $): $ P(X=1)=p $, $ P(X=0)=1-p $
- Binomial($ n, p $): $ P(X = k) = \binom{n}{k} p^k (1-p)^{n-k} $
- Poisson($ \lambda $): $ P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!} $

### Continuous Case

Probability density function (pdf) $ f(x) $:

$$
P(a \leq X \leq b) = \int_a^b f(x) \, dx
$$

$$
\int_{-\infty}^{\infty} f(x) \, dx = 1
$$

Examples:
- Uniform($ a, b $): $ f(x) = \frac{1}{b - a} $
- Normal($ \mu, \sigma^2 $):
$$
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
$$

## 2.4 Expected Value and Moments

### Expected Value (Mean)

- Discrete: $ \mathbb{E}[X] = \sum x_i p(x_i) $
- Continuous: $ \mathbb{E}[X] = \int_{-\infty}^{\infty} x f(x) dx $

### Variance

$$
\text{Var}(X) = \mathbb{E}[(X - \mu)^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

### Higher-Order Moments

- $ r $-th raw moment: $ \mu'_r = \mathbb{E}[X^r] $
- $ r $-th central moment: $ \mu_r = \mathbb{E}[(X - \mu)^r] $
- Skewness: $ \gamma_1 = \frac{\mu_3}{\sigma^3} $
- Kurtosis: $ \gamma_2 = \frac{\mu_4}{\sigma^4} $

## 2.5 Important Theorems

### Law of Large Numbers (LLN)

If $ X_1, \dots, X_n $ are i.i.d. with $ \mathbb{E}[X_i] = \mu $:

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{a.s.} \mu
$$

### Central Limit Theorem (CLT)

If $ X_i $ are i.i.d. with mean $ \mu $, variance $ \sigma^2 $:

$$
\frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} \xrightarrow{d} \mathcal{N}(0, 1)
$$

## 2.6 Joint, Marginal, and Conditional Distributions

Let $ X, Y $ be random variables.

### Joint Distribution

- Discrete: $ p(x, y) = P(X = x, Y = y) $
- Continuous: $ f(x, y) $ with:
$$
P((X, Y) \in A) = \iint_A f(x, y)\, dx\,dy
$$

### Marginal Distribution

$$
f_X(x) = \int f(x, y) \, dy, \quad f_Y(y) = \int f(x, y) \, dx
$$

### Conditional Distribution

$$
f_{X \mid Y}(x \mid y) = \frac{f(x, y)}{f_Y(y)}
$$
