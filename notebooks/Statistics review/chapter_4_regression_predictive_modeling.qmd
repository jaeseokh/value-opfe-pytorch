
---
title: "Chapter 4: Regression and Predictive Modeling"
format: 
  pdf:
    toc: true
    number-sections: true
    number-depth: 2
    fig-cap-location: bottom
    fig-align: center
    keep-tex: true
    link-citations: true
    reference-location: document
    citecolor: blue
---

# 4. Regression and Predictive Modeling (Mathematical Foundations)

This chapter formalizes regression analysis as a predictive modeling framework. We begin with the classical linear model and extend to regularization and predictive performance evaluation.

## 4.1 Linear Regression Model

### Model Specification

Let $Y \in \mathbb{R}^n$, $X \in \mathbb{R}^{n \times p}$:

$$
Y = X\beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n)
$$

- $Y$: response vector  
- $X$: predictor matrix  
- $\beta$: coefficients  
- $\varepsilon$: error (mean 0, constant variance)

### Least Squares Estimator (OLS)

Minimize RSS:

$$
\hat{\beta}_{\text{OLS}} = \arg\min_{\beta} \| Y - X\beta \|_2^2
\Rightarrow
\hat{\beta}_{\text{OLS}} = (X^\top X)^{-1} X^\top Y
$$

### Properties

- Unbiased: $\mathbb{E}[\hat{\beta}] = \beta$
- Variance: $\text{Var}(\hat{\beta}) = \sigma^2 (X^\top X)^{-1}$
- BLUE (Gauss-Markov)

## 4.2 Model Diagnostics & Assumptions

1. Linearity: $Y = X\beta + \varepsilon$  
2. Exogeneity: $\mathbb{E}[\varepsilon \mid X] = 0$  
3. Homoscedasticity: $\text{Var}(\varepsilon_i) = \sigma^2$  
4. No multicollinearity: $X^\top X$ invertible  
5. Normality (for inference): $\varepsilon \sim \mathcal{N}(0, \sigma^2 I)$

## 4.3 Regularization

### Ridge Regression

$$
\hat{\beta}_{\text{ridge}} = \arg\min_\beta \left\{ \| Y - X\beta \|_2^2 + \lambda \| \beta \|_2^2 \right\}
= (X^\top X + \lambda I)^{-1} X^\top Y
$$

### Lasso Regression

$$
\hat{\beta}_{\text{lasso}} = \arg\min_\beta \left\{ \| Y - X\beta \|_2^2 + \lambda \| \beta \|_1 \right\}
$$

### Elastic Net

$$
\hat{\beta}_{\text{EN}} = \arg\min_\beta \left\{ \| Y - X\beta \|_2^2 + \lambda_1 \| \beta \|_1 + \lambda_2 \| \beta \|_2^2 \right\}
$$

## 4.4 Model Evaluation

### Bias-Variance Decomposition

$$
\mathbb{E}[(Y - \hat{f}(x))^2] = [\mathbb{E}[\hat{f}(x)] - f(x)]^2 + \text{Var}(\hat{f}(x)) + \sigma^2
$$

### Cross-Validation (K-fold)

- Split into $K$ folds
- Train on $K-1$, test on 1
- Average error

### Metrics

- MSE: $\frac{1}{n} \sum (y_i - \hat{y}_i)^2$
- RMSE: $\sqrt{\text{MSE}}$
- MAE: $\frac{1}{n} \sum |y_i - \hat{y}_i|$
- $R^2$: proportion of variance explained

## 4.5 Generalized Linear Models (GLM)

Link function $g(\cdot)$ transforms mean:

$$
g(\mathbb{E}[Y \mid X]) = X\beta
$$

Examples:
- Logistic: $g(p) = \log\left(\frac{p}{1 - p}\right)$
- Poisson: $g(\mu) = \log(\mu)$
